<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Seungyeon Yoo</title>

    <meta name="author" content="Seungyeon Yoo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J1YQDL9FWH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-J1YQDL9FWH');
    </script>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Seungyeon Yoo
                </p>
                <p>
		<!-- I'm a research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>.
		At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>, and <a href="https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/">Shopping</a>.
		I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>.
		I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
    I am a PhD candidate at Seoul National University, advised by <a href="https://larr.snu.ac.kr/">Prof. H. Jin Kim</a>.
    My research interests include vision-based navigation, visual learning, and robotics.
                </p>
                <p style="text-align:center">
                  <a href="mailto:syeon.yoo@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/SeungyeonYoo-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/SeungyeonYoo-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://github.com/jonbarron/">Github</a> -->
                  <a href="https://linkedin.com/in/ysy703">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/SeungyeonYoo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/SeungyeonYoo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  <!-- I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>. -->
                  My research focuses on building RGB-only autonomous robot systems. I have been exploring GS-based real-to-sim data generation to address the sim-to-real gap, monocular-camera-based target chasing through cross-modal learning, and single-view RL manipulation leveraging NeRF.
                  I apply these approaches across diverse robot platforms, including drones, manipulators, and ground robots.
                  I believe that RGB-focused robot systems can serve as a key pathway toward realizing human-level intelligence in robotics.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="sincro_stop()" onmouseover="sincro_start()">
      <td style="padding:16px;width:20%;vertical-align:middle;">
        <div class="one">
          <div class="two" id='sincro_image'>
            <video style="object-fit: cover;" width=100% height=100% muted autoplay loop>
              <source src="images/sincro.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video></div>
          <img src='images/sincro.png' width="160">
        </div>
        <script type="text/javascript">
          function sincro_start() {
            document.getElementById('sincro_image').style.opacity = "1";
          }

          function sincro_stop() {
            document.getElementById('sincro_image').style.opacity = "0";
          }
          sincro_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://sincro-ral.github.io/">
          <span class="papertitle">Single-View 3D-Aware Representations for Reinforcement Learning by Cross-View Neural Radiance Fields</span>
        </a>
        <br>
        Daesol Cho*,
		    <strong>Seungyeon Yoo*</strong>,
        Dongseok Shim, H. Jin Kim
        <br>
        <em>RA-L</em>, 2025
        <br>
        <a href="https://sincro-ral.github.io/">project page</a>
        <!-- /
        <a href="https://szymanowiczs.github.io/bolt3d">arXiv</a> -->
        <p></p>
        <p>
		    We propose a novel RL framework that extracts 3D-aware representations from single-view RGB input, without requiring camera pose or synchronized multi-view images during the downstream RL.
        </p>
      </td>
    </tr>

    <tr onmouseout="cmchasing_stop()" onmouseover="cmchasing_start()">
      <td style="padding:16px;width:20%;vertical-align:middle;">
        <div class="one">
          <div class="two" id='cmchasing_image'>
            <video style="object-fit: cover;" width=100% height=100% muted autoplay loop>
              <source src="images/cmchasing_mini.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <img src='images/cmchasing.png' width="160">
        </div>
        <script type="text/javascript">
          function cmchasing_start() {
            document.getElementById('cmchasing_image').style.opacity = "1";
          }

          function cmchasing_stop() {
            document.getElementById('cmchasing_image').style.opacity = "0";
          }
          cmchasing_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10542210">
			<span class="papertitle">Mono-Camera-Only Target Chasing for a Drone in a Dense Environment by Cross-Modal Learning
</span>
        </a>
        <br>
				<strong>Seungyeon Yoo*</strong>,
        Seungwoo Jung*, Yunwoo Lee, Dongseok Shim, H. Jin Kim
				<br>
        <em>RA-L</em>, 2025
        <br>
        <a href="https://sites.google.com/view/cm-chasing">project page</a>
        /
        <a href="https://ieeexplore.ieee.org/document/10542210">paper</a>
        <p></p>
        <p>
				Cross-modal representation enables RGB-only target chasing instead of multiple sensor inputs.
        </p>
      </td>
    </tr>
            



          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <!-- Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
                  This page was built with <a href="https://github.com/jonbarron/jonbarron_website">Dr. Jon Barron's</a> template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
